{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranesh008/next-word-prediction/blob/main/Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4f7Qb-VTCsd",
        "outputId": "cbc8660d-7815-44c1-f270-cf55bec6f90b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import string\n",
        "import random\n",
        "import nltk\n",
        "\n",
        "from nltk.probability import ConditionalFreqDist\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SfNAot9Tj9G",
        "outputId": "9c1c5244-a37a-4336-fadb-7d9a7f4c85de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtering...\n",
            "Cleaning...\n",
            "Making model...\n",
            "Enter a phrase: \n",
            "what are \n",
            "Trigram model predictions:  ['you', 'they', 'your', 'tart']\n",
            "what are tart\n",
            "Do you want to generate another word? (type 'y' for yes or 'n' for no): y\n",
            "Trigram model predictions:  ['made']\n",
            "what are tart made\n",
            "Do you want to generate another word? (type 'y' for yes or 'n' for no): y\n",
            "Trigram model predictions:  ['of']\n",
            "what are tart made of\n",
            "Do you want to generate another word? (type 'y' for yes or 'n' for no): y\n",
            "Trigram model predictions:  ['solid', 'alice', 'pepper']\n",
            "what are tart made of solid\n",
            "Do you want to generate another word? (type 'y' for yes or 'n' for no): y\n",
            "Trigram model predictions:  ['glass']\n",
            "what are tart made of solid glass\n",
            "Do you want to generate another word? (type 'y' for yes or 'n' for no): y\n",
            "Trigram model predictions:  ['there']\n",
            "what are tart made of solid glass there\n",
            "Do you want to generate another word? (type 'y' for yes or 'n' for no): y\n",
            "Trigram model predictions:  ['wa']\n",
            "what are tart made of solid glass there wa\n",
            "Do you want to generate another word? (type 'y' for yes or 'n' for no): no\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    file = open('/content/corpus.txt', 'r')\n",
        "    text = \"\"\n",
        "    while True:\n",
        "        line = file.readline()\n",
        "        text += line\n",
        "        if not line:\n",
        "            break\n",
        "\n",
        "\n",
        "    # pre-process text\n",
        "    print(\"Filtering...\")\n",
        "    words = filter(text)\n",
        "    print(\"Cleaning...\")\n",
        "    words = clean(words)\n",
        "\n",
        "    # make language model\n",
        "    print(\"Making model...\")\n",
        "    model = n_gram_model(words)\n",
        "\n",
        "    print(\"Enter a phrase: \")\n",
        "    user_input = input()\n",
        "    predict(model, user_input)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    Normalize text, remove unnecessary characters,\n",
        "    perform regex parsing, and make lowercase\n",
        "\"\"\"\n",
        "\n",
        "def filter(text):\n",
        "    # normalize text\n",
        "    text = (unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore'))\n",
        "    # replace html chars with ' '\n",
        "    text = re.sub('<.*?>', ' ', text)\n",
        "    # remove punctuation\n",
        "    text = text.translate(str.maketrans(' ', ' ', string.punctuation))\n",
        "    # only alphabets and numerics\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    # replace newline with space\n",
        "    text = re.sub(\"\\n\", \" \", text)\n",
        "    # lower case\n",
        "    text = text.lower()\n",
        "    # split and join the words\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text\n",
        "\n",
        "\"\"\"\n",
        "    Tokenize remaining words\n",
        "    and perform lemmatization\n",
        "\"\"\"\n",
        "def clean(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    wnl = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "    output = []\n",
        "    for words in tokens:\n",
        "        # lemmatize words\n",
        "        output.append(wnl.lemmatize(words))\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    Make a language model using a dictionary, trigrams,\n",
        "    and calculate word probabilities\n",
        "\"\"\"\n",
        "def n_gram_model(text):\n",
        "    trigrams = list(nltk.ngrams(text, 3, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
        "    # bigrams = list(nltk.ngrams(text, 2, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
        "\n",
        "# N-gram Statistics\n",
        "    # get freq dist of trigrams\n",
        "    # freq_tri = nltk.FreqDist(trigrams)\n",
        "    # freq_bi = nltk.FreqDist(bigrams)\n",
        "    # freq_tri.plot(30, cumulative=False)\n",
        "    # print(\"Most common trigrams: \", freq_tri.most_common(5))\n",
        "    # print(\"Most common bigrams: \", freq_bi.most_common(5))\n",
        "\n",
        "    # make conditional frequencies dictionary\n",
        "    cfdist = ConditionalFreqDist()\n",
        "    for w1, w2, w3 in trigrams:\n",
        "        cfdist[(w1, w2)][w3] += 1\n",
        "\n",
        "    # transform frequencies to probabilities\n",
        "    for w1_w2 in cfdist:\n",
        "        total_count = float(sum(cfdist[w1_w2].values()))\n",
        "        for w3 in cfdist[w1_w2]:\n",
        "            cfdist[w1_w2][w3] /= total_count\n",
        "\n",
        "    return cfdist\n",
        "\n",
        "\"\"\"\n",
        "    Generate predictions from the Conditional Frequency Distribution\n",
        "    dictionary (param: model), append weighted random choice to user's phrase,\n",
        "    allow option to generate more words following the prediction\n",
        "\"\"\"\n",
        "def predict(model, user_input):\n",
        "    user_input = filter(user_input)\n",
        "    user_input = user_input.split()\n",
        "\n",
        "    if len(user_input) < 2:\n",
        "        # If the user input has less than 2 words, use the padding symbols instead\n",
        "        prev_words = ['<s>', '<s>']\n",
        "    else:\n",
        "        w1 = len(user_input) - 2\n",
        "        w2 = len(user_input)\n",
        "        prev_words = user_input[w1:w2]\n",
        "\n",
        "    # display prediction from highest to lowest maximum likelihood\n",
        "    try:\n",
        "        prediction = sorted(dict(model[prev_words[0], prev_words[1]]), key=lambda x: dict(model[prev_words[0], prev_words[1]])[x], reverse=True)\n",
        "    except KeyError:\n",
        "        # If the model doesn't have a prediction for the given previous words, use the padding symbols\n",
        "        prediction = ['<s>', '</s>']\n",
        "\n",
        "    if not prediction:\n",
        "        print(\"Trigram model predictions: No predictions available.\")\n",
        "        return\n",
        "\n",
        "    print(\"Trigram model predictions: \", prediction)\n",
        "\n",
        "    word = []\n",
        "    weight = []\n",
        "    for key, prob in dict(model[prev_words[0], prev_words[1]]).items():\n",
        "        word.append(key)\n",
        "        weight.append(prob)\n",
        "\n",
        "    # If the prediction list is empty, don't generate a new word\n",
        "    if not word:\n",
        "        print(' '.join(user_input))\n",
        "        ask = input(\"Do you want to generate another word? (type 'y' for yes or 'n' for no): \")\n",
        "        if ask.lower() == 'y':\n",
        "            predict(model, str(user_input))\n",
        "        elif ask.lower() == 'n':\n",
        "            print(\"done\")\n",
        "        return\n",
        "\n",
        "    # pick from a weighted random probability of predictions\n",
        "    next_word = random.choices(word, weights=weight, k=1)\n",
        "    # add predicted word to user input\n",
        "    user_input.append(next_word[0])\n",
        "    print(' '.join(user_input))\n",
        "\n",
        "    ask = input(\"Do you want to generate another word? (type 'y' for yes or 'n' for no): \")\n",
        "    if ask.lower() == 'y':\n",
        "        predict(model, str(user_input))\n",
        "    elif ask.lower() == 'n':\n",
        "        print(\"done\")\n",
        "\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7ykH-LFUWDF",
        "outputId": "80d0f56e-aeaa-4a68-bfef-e451b55777f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtering...\n",
            "Cleaning...\n",
            "Making model...\n",
            "Enter a phrase: \n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\"\"\"\n",
        "    Normalize text, remove unnecessary characters,\n",
        "    perform regex parsing, and make lowercase\n",
        "\"\"\"\n",
        "def filter(text):\n",
        "    # normalize text\n",
        "    text = (unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore'))\n",
        "    # replace html chars with ' '\n",
        "    text = re.sub('<.*?>', ' ', text)\n",
        "    # remove punctuation\n",
        "    text = text.translate(str.maketrans(' ', ' ', string.punctuation))\n",
        "    # only alphabets and numerics\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    # replace newline with space\n",
        "    text = re.sub(\"\\n\", \" \", text)\n",
        "    # lower case\n",
        "    text = text.lower()\n",
        "    # split and join the words\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text\n",
        "\n",
        "\"\"\"\n",
        "    Tokenize remaining words\n",
        "    and perform lemmatization\n",
        "\"\"\"\n",
        "def clean(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    wnl = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "    output = []\n",
        "    for words in tokens:\n",
        "        # lemmatize words\n",
        "        output.append(wnl.lemmatize(words))\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    Make a language model using a dictionary, trigrams,\n",
        "    and calculate word probabilities\n",
        "\"\"\"\n",
        "def n_gram_model(text):\n",
        "    trigrams = list(nltk.ngrams(text, 3, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
        "    # bigrams = list(nltk.ngrams(text, 2, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
        "\n",
        "# N-gram Statistics\n",
        "    # get freq dist of trigrams\n",
        "    # freq_tri = nltk.FreqDist(trigrams)\n",
        "    # freq_bi = nltk.FreqDist(bigrams)\n",
        "    # freq_tri.plot(30, cumulative=False)\n",
        "    # print(\"Most common trigrams: \", freq_tri.most_common(5))\n",
        "    # print(\"Most common bigrams: \", freq_bi.most_common(5))\n",
        "\n",
        "    # make conditional frequencies dictionary\n",
        "    cfdist = ConditionalFreqDist()\n",
        "    for w1, w2, w3 in trigrams:\n",
        "        cfdist[(w1, w2)][w3] += 1\n",
        "\n",
        "    # transform frequencies to probabilities\n",
        "    for w1_w2 in cfdist:\n",
        "        total_count = float(sum(cfdist[w1_w2].values()))\n",
        "        for w3 in cfdist[w1_w2]:\n",
        "            cfdist[w1_w2][w3] /= total_count\n",
        "\n",
        "    return cfdist\n",
        "\n",
        "\"\"\"\n",
        "    Generate predictions from the Conditional Frequency Distribution\n",
        "    dictionary (param: model), append weighted random choice to user's phrase,\n",
        "    allow option to generate more words following the prediction\n",
        "\"\"\"\n",
        "def predict(model, user_input):\n",
        "    user_input = filter(user_input)\n",
        "    user_input = user_input.split()\n",
        "\n",
        "    if len(user_input) < 2:\n",
        "        # If the user input has less than 2 words, use the padding symbols instead\n",
        "        prev_words = ['<s>', '<s>']\n",
        "    else:\n",
        "        w1 = len(user_input) - 2\n",
        "        w2 = len(user_input)\n",
        "        prev_words = user_input[w1:w2]\n",
        "\n",
        "    # display prediction from highest to lowest maximum likelihood\n",
        "    try:\n",
        "        prediction = sorted(dict(model[prev_words[0], prev_words[1]]), key=lambda x: dict(model[prev_words[0], prev_words[1]])[x], reverse=True)\n",
        "    except KeyError:\n",
        "        # If the model doesn't have a prediction for the given previous words, use the padding symbols\n",
        "        prediction = ['<s>', '</s>']\n",
        "\n",
        "    if not prediction:\n",
        "        print(\"Trigram model predictions: No predictions available.\")\n",
        "        return\n",
        "\n",
        "    print(\"Trigram model predictions: \", prediction)\n",
        "\n",
        "    word = []\n",
        "    weight = []\n",
        "    for key, prob in dict(model[prev_words[0], prev_words[1]]).items():\n",
        "        word.append(key)\n",
        "        weight.append(prob)\n",
        "\n",
        "    # If the prediction list is empty, don't generate a new word\n",
        "    if not word:\n",
        "        print(' '.join(user_input))\n",
        "        ask = input(\"Do you want to generate another word? (type 'y' for yes or 'n' for no): \")\n",
        "        if ask.lower() == 'y':\n",
        "            predict(model, str(user_input))\n",
        "        elif ask.lower() == 'n':\n",
        "            print(\"done\")\n",
        "        return\n",
        "\n",
        "    # pick from a weighted random probability of predictions\n",
        "    next_word = random.choices(word, weights=weight, k=1)\n",
        "    # add predicted word to user input\n",
        "    user_input.append(next_word[0])\n",
        "    print(' '.join(user_input))\n",
        "\n",
        "    ask = input(\"Do you want to generate another word? (type 'y' for yes or 'n' for no): \")\n",
        "    if ask.lower() == 'y':\n",
        "        predict(model, str(user_input))\n",
        "    elif ask.lower() == 'n':\n",
        "        print(\"done\")\n",
        "\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llmEJA9YkdOY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOK8CW5bEONtkagnyM6NxVG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}